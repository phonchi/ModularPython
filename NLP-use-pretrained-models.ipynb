{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuOXkMR73rhs"
      },
      "source": [
        "# NLP: Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/phonchi/ModularPython/blob/master/NLP-use-pretrained-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/phonchi/ModularPython/blob/master/NLP-use-pretrained-models.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gHdfxlxuvo3"
      },
      "source": [
        "This notebook is adapted by [Haowen Jiang](https://howard-haowen.rohan.tw/) from [this one](https://github.com/nlptown/nlp-notebooks/blob/master/NLP%20with%20pretrained%20models%20-%20spaCy%20and%20StanfordNLP.ipynb) included in the [nlptown\n",
        "/nlp-notebooks](https://github.com/nlptown/nlp-notebooks) repo. It is meant for the 2022 [NLP Workshop at NSYSU](https://howard-haowen.github.io/NLP-demos/nsysu_workshop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUnEs1QovufO"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "today = date.today()\n",
        "print(\"Last updated:\", today)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGALGAK2wk-0"
      },
      "source": [
        "# 📘 NLP with pretrained models - spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T0bh4HV62Vo"
      },
      "outputs": [],
      "source": [
        "# @title spaCy Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    #!pip install -U pip setuptools wheel -qq\n",
        "    #!pip install -U spacy -qq\n",
        "    !python -m spacy download en_core_web_md -qq # downloads the medium-sized English language model\n",
        "    !python -m spacy download zh_core_web_md -qq # downloads the medium-sized Chinese language model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://spacy.io/images/pipeline.svg)"
      ],
      "metadata": {
        "id": "Pu9hGUkN1toE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG9Fr9tMwk-5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ0etPoz2Xuy"
      },
      "outputs": [],
      "source": [
        "spacy.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUXK0igWCdoq"
      },
      "source": [
        "- To get you started, play with [this Web App](https://share.streamlit.io/howard-haowen/spacy-streamlit/app.py) that I created, which is powered by spaCy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDns3sBg2rvZ"
      },
      "source": [
        "## English NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy_QA0ib7hE1"
      },
      "outputs": [],
      "source": [
        "en = spacy.load(\"en_core_web_md\") # Loading the spaCy Model which includes vocabulary, syntax models, and entities.\n",
        "df_metadata = pd.DataFrame([en.meta])\n",
        "df_metadata.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlVecKvxwk-7"
      },
      "outputs": [],
      "source": [
        "text = (\"Donald John Trump (born June 14, 1946) is the 45th and previous president of \"\n",
        "     \"the United States.  Before entering politics, he was a businessman and television personality.\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQcxTvqj6M8y"
      },
      "source": [
        "Here, the text about Donald Trump is processed by the spaCy model, creating a `Doc` object (A `Doc` object is a sequence of Token objects representing a lexical token) `doc_en` that contains all the information about the text's structure and content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l3k7kszwk-7"
      },
      "outputs": [],
      "source": [
        "doc_en = en(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g_rM0qIwk-8"
      },
      "outputs": [],
      "source": [
        "tokens = [token.text for token in doc_en]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSv5WA3twk-8"
      },
      "source": [
        "SpaCy also splits your document into sentences. In spaCy, the `.sents` property is used to extract sentences from the Doc object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IduCRRqwk-8"
      },
      "outputs": [],
      "source": [
        "sentences = list(doc_en.sents)\n",
        "len(sentences), sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-Speech tagging"
      ],
      "metadata": {
        "id": "8Ox9qEGThLjv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hokEXPh0wk-9"
      },
      "source": [
        "In addition, spaCy identifies a variety of linguistic features for each token. Among the foundational features are the lemma and two types of parts-of-speech (POS) tags. The `pos_` attribute encompasses the [Universal POS tags](https://universaldependencies.org/u/pos/) derived from the [Universal Dependencies](https://universaldependencies.org/) framework, which provide a consistent categorization of word types across languages. On the other hand, the `tag_` attribute offers more detailed, language-specific POS tags that capture finer grammatical distinctions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence."
      ],
      "metadata": {
        "id": "VSh8N2w6ghMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fQ81Ye9wk-_"
      },
      "outputs": [],
      "source": [
        "# orthographic representation, lemma, coarse-grained part-of-speech (pos_), and fine-grained part-of-speech (tag_).\n",
        "features = [\n",
        "    {'Text': token.orth_, 'Lemma': token.lemma_, 'POS': token.pos_, 'Detailed POS': token.tag_, 'Explain': spacy.explain(token.tag_)}\n",
        "    for token in doc_en\n",
        "]\n",
        "\n",
        "df_features = pd.DataFrame(features)\n",
        "df_features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named-Entity Recognition"
      ],
      "metadata": {
        "id": "Al-MsZXMhU7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ZjaXBfwk_A"
      },
      "source": [
        "Next, spaCy includes pre-trained models for named entity recognition (NER). The outcomes of these models are reflected in the `ent_iob_` and `ent_type` attributes. The `ent_type` attribute specifies the category of entity identified by the model, such as a person, date, ordinal number, or geopolitical entity (GPE). For instance, in English models adhering to the [OntoNotes standard](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf), \"Donald John Trump\" is recognized as a person, \"June 14, 1946\" as a date, \"45th\" as an ordinal number, and \"the United States\" as a GPE.\n",
        "\n",
        "The `ent_iob_` attribute (inside-outside-beginning (IOB) tagging) indicates the token's position within an entity: `O` for outside any entity, `B` for the beginning of an entity, and `I` for inside an entity (but not at the beginning). This notation is part of the `BIO` tagging scheme, which helps differentiate between consecutive entities of the same type.\n",
        "\n",
        "> Other schemes like `BILUO` include additional designations for the last token of an entity and for unique, standalone entity tokens, providing detailed positional information within entity sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-es444kwk_A"
      },
      "outputs": [],
      "source": [
        "# Extracting named entity information from each token in the document\n",
        "entities = [\n",
        "    {'Text': token.orth_, 'IOB Tag': token.ent_iob_, 'Entity Type': token.ent_type_, 'Explain': spacy.explain(token.ent_type_)} #_ is to get the string\n",
        "    for token in doc_en  # Iterate over each token\n",
        "]\n",
        "\n",
        "df_entities = pd.DataFrame(entities)\n",
        "df_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAnHlup7wk_B"
      },
      "source": [
        "You can also access the entities directly on the `ents` attribute of the document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eguxaqnVwk_B"
      },
      "outputs": [],
      "source": [
        "print([(ent.text, ent.label_) for ent in doc_en.ents])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Parsing"
      ],
      "metadata": {
        "id": "YLTGlUldjYZm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_lp-ih4wk_B"
      },
      "source": [
        "spaCy also contains a dependency parser, which analyzes the grammatical relations between the tokens.\n",
        "\n",
        "> Dependency parsing is the process of extracting the dependency graph of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the root of the sentence. All other words are linked to the headword. Specifically,the dependencies can be mapped in a directed graph representation where words are the nodes and grammatical relationships are the edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZHHNKwTwk_C"
      },
      "outputs": [],
      "source": [
        "# Extracting syntax or dependency parsing information from each token\n",
        "syntax = [\n",
        "    {'Token': token.text, 'Dependency': token.dep_, 'Head': token.head.text, 'Explain': spacy.explain(token.dep_)}\n",
        "    for token in doc_en  # Iterate over each token in the document\n",
        "]\n",
        "\n",
        "df_syntax = pd.DataFrame(syntax)\n",
        "df_syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umAUHhTdQU8b"
      },
      "source": [
        "Finally, the English spaCy model contains a morphological parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpFmyJaVQVm2"
      },
      "outputs": [],
      "source": [
        "# Extracting morphological features from each token in the document\n",
        "features = [\n",
        "    {'Token': token.text, 'Morphological Features': token.morph}\n",
        "    for token in doc_en  # Iterate over each token\n",
        "]\n",
        "\n",
        "df_features = pd.DataFrame(features)\n",
        "df_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLClbFviwk_C"
      },
      "source": [
        "## Multilingual NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbp79ilwk_C"
      },
      "source": [
        "SpaCy doesn't only have models for English, but also for many other languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5zp8GAz8hxh"
      },
      "outputs": [],
      "source": [
        "zh = spacy.load(\"zh_core_web_md\")\n",
        "df_metadata = pd.DataFrame([en.meta])\n",
        "df_metadata.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HemfWhawk_C"
      },
      "outputs": [],
      "source": [
        "text_zh = \"中山大學人文暨科技跨領域學士學位學程助理教授宋世祥表示，2021年聖誕節假期期間，師生舉辦「街頭玩童～鹽埕兒童街區遊戲日」成果展。活動中可看見學生運用贊助單位瑞儀教育基金會致贈的廢棄木棧板，製作了6具兒童創意遊具，一方面展示學習成果，也希望藉此呼籲高雄民眾重視兒童的遊戲權。\"\n",
        "doc_zh = zh(text_zh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsF4wHjwwk_D"
      },
      "source": [
        "The tokens in the Chinese document share the same attribute structure as those in the English document in spaCy. However, the functionalities of the models can vary significantly between languages. One key difference to note is in the handling of lemmatization:\n",
        "\n",
        "- **Lack of Lemmatization in Chinese Model**: Unlike the English model, the Chinese model does not provide lemmatization.\n",
        "\n",
        "This distinction is important to consider when performing text processing tasks, as it affects the depth of linguistic analysis available for each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AogpQPsO8DRR"
      },
      "outputs": [],
      "source": [
        "list(doc_zh.sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7uMi5669USU"
      },
      "outputs": [],
      "source": [
        "tok_text = [tok.text for tok in doc_zh]\n",
        "tok_orth = [tok.orth_ for tok in doc_zh]\n",
        "print(tok_text)\n",
        "print(tok_orth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zf3TSWY95hc"
      },
      "outputs": [],
      "source": [
        "for tok in list(doc_zh.sents)[1]: # The second sentence\n",
        "    print(f\"{tok.text} >>> {tok.pos_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8SN7SUAX8y"
      },
      "source": [
        "- The Chinese model has a very different fine-grained part-of-speech tags on the `tag_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgpNWCl4AAke"
      },
      "outputs": [],
      "source": [
        "# Printing each token's text, detailed POS tag, and an explanation of the tag.\n",
        "for tok in list(doc_zh.sents)[1]:\n",
        "    print(f\"{tok.text} >>> {tok.tag_} | {spacy.explain(tok.tag_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZmsq_gYAc0T"
      },
      "source": [
        "- The Chinese model has different entity types (PER, LOC and ORG) than the English one.\n",
        "\n",
        "This is a result of the training corpora that were used to build the models, whose annotation guidelines may be very different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3hIsara5th9"
      },
      "outputs": [],
      "source": [
        "info = [(t.text, t.pos_, t.tag_, t.ent_iob_, t.ent_type_) for t in doc_zh]\n",
        "df_info = pd.DataFrame(info, columns=['Text', 'POS', 'Tag', 'IOB Tag', 'Entity Type'])\n",
        "df_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GyNQ4UjDqTH"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoENmU8oBQMt"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNGB9n6nDuTE"
      },
      "outputs": [],
      "source": [
        "displacy.render(doc_zh, style='ent',jupyter=True, options={'distance':130})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bZWBpW8BR73"
      },
      "outputs": [],
      "source": [
        "text = \"我想要三份2號餐\"\n",
        "doc = zh(text)\n",
        "displacy.render(doc, style='dep',jupyter=True, options={'distance':130})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-A6lmiG_1Yd"
      },
      "source": [
        "## DataFrame + spaCy = dframcy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQBGx3gB-sQK"
      },
      "outputs": [],
      "source": [
        "# @title dframcy Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    !pip install dframcy -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0sexga0-yWU"
      },
      "outputs": [],
      "source": [
        "from dframcy import DframCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDO97xhS-pnj"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('zh_core_web_md')\n",
        "# Initialize DframCy with the spaCy NLP model to integrate with pandas DataFrame.\n",
        "dframcy = DframCy(nlp)\n",
        "# Process the Chinese text using the NLP model to create a spaCy document.\n",
        "doc = dframcy.nlp(text_zh)\n",
        "# Convert the NLP document annotations to a pandas DataFrame for easier analysis.\n",
        "annotation_dataframe = dframcy.to_dataframe(doc)\n",
        "annotation_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7u0msqWA2Km"
      },
      "source": [
        "Once annotations are stored as a DataFrame object, filtering can be easily done by leveraging the power of `pandas` syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK2eJJojEf_I"
      },
      "outputs": [],
      "source": [
        "# Create a filter for rows where the part-of-speech tag is 'NN' (noun).\n",
        "nn_filt = annotation_dataframe['token_tag_'] == 'NN'\n",
        "# Create a filter for rows where the dependency label is 'dobj' (direct object).\n",
        "dobj_filt = annotation_dataframe['token_dep_'] == 'dobj'\n",
        "# Get rows where the token is a noun and serves as a direct object.\n",
        "annotation_dataframe[(nn_filt) & dobj_filt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCpCdxLhF0Pq"
      },
      "source": [
        "## Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2120c0njF2T1"
      },
      "outputs": [],
      "source": [
        "doc = zh(\"教授\")\n",
        "tok = doc[0]\n",
        "tok.vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX_NjkK1F-mP"
      },
      "outputs": [],
      "source": [
        "tok.vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    # Ensure the vectors are not only zeros\n",
        "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
        "        return 0.0\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "\n",
        "# Define the words and retrieve vectors\n",
        "word_1 = zh(\"高興\").vector\n",
        "word_2 = zh(\"高雄\").vector\n",
        "word_3 = zh(\"開心\").vector\n",
        "\n",
        "# Calculate similarities\n",
        "word_1_word_2_similarity = cosine_similarity(word_1, word_2)\n",
        "word_1_word_3_similarity = cosine_similarity(word_1, word_3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Distance between '高興' and '高雄': {word_1_word_2_similarity}\")\n",
        "print(f\"Distance between '高興' and '開心': {word_1_word_3_similarity}\")"
      ],
      "metadata": {
        "id": "z8Rvx7YmA5HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTJbrvaAGNvq"
      },
      "source": [
        "- Cosine similarity\n",
        "\n",
        "![](https://zhangruochi.com/Operations-on-word-vectors-Debiasing/2019/03/28/images/cosine_sim.png)\n",
        "\n",
        "- Formula for calculating cosine similarity between two vectors\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*LfW66-WsYkFqWc4XYJbEJg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04olEHqfwk_D"
      },
      "source": [
        "## 🔍 Supplementary: StanfordNLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghzefh-xwk_D"
      },
      "source": [
        "Another library that shares some functionality with spaCy is StanfordNLP. [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/), distinct from Stanford’s Java-based [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) library, is a [Python library](https://github.com/stanfordnlp/stanfordnlp) developed on the PyTorch framework. It provides a fully neural NLP pipeline, which includes advanced features such as tokenization (capable of recognizing multi-word units), lemmatization, part-of-speech tagging (incorporating morphological features), and state-of-the-art dependency parsing. These components were specifically designed and trained for the [CoNLL-2018 shared task](https://nlp.stanford.edu/pubs/qi2018universal.pdf). While it does not include named entity recognition, StanfordNLP excels in dependency parsing and additionally offers a Python interface to CoreNLP, facilitating integration into Python projects.\n",
        "\n",
        "This version provides a clearer distinction between the two Stanford libraries and emphasizes the specific capabilities and strengths of StanfordNLP, enhancing the reader's understanding of its purpose and utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9UtQCFzwk_D"
      },
      "source": [
        "> **`stanfordnlp` has been renamed to `stanza`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZncAMak2G-0S"
      },
      "outputs": [],
      "source": [
        "# @title stanza Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    !pip install stanza -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ct8iirDHhUa"
      },
      "outputs": [],
      "source": [
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7S29T-ZKjWg"
      },
      "outputs": [],
      "source": [
        "stanza.download(\"zh-hant\") # Download the traditional Chinese model for Stanza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5NFJi_WMByF"
      },
      "outputs": [],
      "source": [
        "stf_nlp = stanza.Pipeline('zh-hant') # Initialize the Stanza pipeline for traditional Chinese to handle various NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmZRFfdYHSnL"
      },
      "outputs": [],
      "source": [
        "text_zh = \"中山大學人文暨科技跨領域學士學位學程助理教授宋世祥表示，2021年聖誕節假期期間，師生舉辦「街頭玩童～鹽埕兒童街區遊戲日」成果展。活動中可看見學生運用贊助單位瑞儀教育基金會致贈的廢棄木棧板，製作了6具兒童創意遊具，一方面展示學習成果，也希望藉此呼籲高雄民眾重視兒童的遊戲權。\"\n",
        "# Process the text with the Stanza pipeline to extract linguistic information.\n",
        "doc = stf_nlp(text_zh)\n",
        "type(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXC7KjTsVFQ6"
      },
      "source": [
        "Different models often produce different tokenization results, which in turn would have impact on POS and DEP tagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kci7Uko7VZcg"
      },
      "source": [
        "- Here're the results based on StandfordNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENBx-6QY74qU"
      },
      "outputs": [],
      "source": [
        "words_data = []\n",
        "for i, sent in enumerate(doc.sentences):\n",
        "    for word in sent.words:\n",
        "        # Prepare and append a dictionary with details about each word to the list.\n",
        "        words_data.append({\n",
        "            'Sentence Number': i + 1,\n",
        "            'Text': word.text,\n",
        "            'Lemma': word.lemma,\n",
        "            'POS': word.pos,\n",
        "            'Head Index': word.head,\n",
        "            'Dependency Relation': word.deprel\n",
        "        })\n",
        "\n",
        "df_words = pd.DataFrame(words_data)\n",
        "df_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE5T8kxWSwmH"
      },
      "source": [
        "## 🔍 Supplementary: Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfDmTaw0TrZ7"
      },
      "source": [
        "### Analyze Enlgish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFjZct1ES--5"
      },
      "source": [
        "- Input: any English news article of your choice\n",
        "- Ouput:\n",
        "    - A list of unique lemmas of all verbs in lower case\n",
        "    - A list of unique tuples of (NER text, NER label)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWg-kuyPXagy"
      },
      "outputs": [],
      "source": [
        "# Change this to any other article of your choice.\n",
        "\n",
        "en_input = \"\"\"\n",
        "Taipei, April 7 (CNA) Health and Welfare Minister Chen Shih-chung (陳時中) said Thursday that COVID-19 contact tracing has been partially suspended in Taiwan and a new disease control model is being put in place, amid a rise in domestic cases.\n",
        "\n",
        "The immediate suspension of contract tracing applies only to travelers who test positive for COVID-19 in Taiwan, either on arrival at the airport or during mandatory quarantine, Chen said.\n",
        "\n",
        "That decision was made in a bid to free up resources to monitor the growing number of domestic COVID-19 cases, he said at a press briefing, after he reported 531 new cases -- 382 domestically transmitted and 149 imported.\n",
        "\n",
        "Chen said contact tracing on new imported cases will only be done if any of them are believed to be linked to COVID-19 clusters at quarantine hotels or quarantine centers in Taiwan.\n",
        "\n",
        "Prior to Thursday, Taiwan had been reporting its contact tracing information on imported COVID-19 cases via the World Health Organization's International Health Regulations (IHR) mechanism, he said.\n",
        "\n",
        "Regarding the recent daily rise in domestic infections, Chen said the current goal is to bring the situation under control, even though it is impossible to achieve zero new domestic cases at this time.\n",
        "\n",
        "Despite the recent spike, the daily number of domestic COVID-19 cases in Taiwan is still low compared to many other countries, he said, citing as an example the 534 new cases per 100,000 population reported in South Korea on Tuesday.\n",
        "\n",
        "Once people in Taiwan stick together and do their part to prevent the spread of the virus, the situation will be manageable, Chen said.\n",
        "\n",
        "Based on the trajectory of COVID-19 Omicron outbreaks observed in many other countries around the world, he said, it is likely that the infections in Taiwan will peak in a month or two.\n",
        "\n",
        "\"We do not expect the outbreak to stop growing now, but we hope it will rise slowly, so that Taiwan's medical capacity will not be overloaded,\" Chen said.\n",
        "\n",
        "Meanwhile, earlier in the day, the Cabinet announced that Taiwan was adopting a new model for the control of COVID-19 infections.\n",
        "\n",
        "Under the \"new Taiwan model,\" the country has let go of its goal to achieve zero COVID-19 cases, but this does not mean allowing the pandemic go unmanaged, Cabinet spokesman Lo Ping-cheng (羅秉成) said, citing Premier Su Tseng-chang (蘇貞昌).\n",
        "\n",
        "In a meeting earlier with Ministry of Health and Welfare (MOHW) officials, Premier Su said that as Taiwan moves towards a new stage of epidemic prevention, he hopes that the central and local governments will work together to gradually open up the country, in the interests of its people and economy, according to Lo.\n",
        "\n",
        "In a report presented to the Cabinet on Thursday, the MOHW said Taiwan will continue to actively manage the COVID-19 situation, while steadily opening up its borders, in consideration of national economic factors and the people's livelihood.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsb37mkWDTlR"
      },
      "outputs": [],
      "source": [
        "# Start by turning a text into a spaCy Doc object\n",
        "en_doc = en(en_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHBSsFhDHahi"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `verbs`.===#\n",
        "\n",
        "\n",
        "# verbs ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_cLlJhODK-K"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `en_ents`.===#\n",
        "\n",
        "# en_ents ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54HIkSkRTzqe"
      },
      "source": [
        "### Analyze Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaHx9Pj-VpPj"
      },
      "source": [
        "- Input 1: any Chinese news article from Taiwan media of your choice\n",
        "- Ouput 1:\n",
        "    - A list of unique tokens except for punctuations\n",
        "    - A list of unique tuples of (NER text, NER label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHfzWXqpXrsI"
      },
      "outputs": [],
      "source": [
        "# Change this to any other article of your choice.\n",
        "\n",
        "zh_input = \"\"\"\n",
        "本土個案昨增三八二例再創新高，確診案例遍及十九縣市，境外增一四九例，單日破五百案例。\n",
        "\n",
        "中央疫情指揮中心指揮官陳時中表示，個案數還會往上升，預估一至兩個月內達最高峰，疫情將持續到六月底且還不會到尾聲；他也首度鬆口「清零不可能」，未來將走向與病毒共存，下周擬試辦「輕症在家」隔離，同時也將調整停課標準。\n",
        "\n",
        "擬以居家快篩取代停課\n",
        "國內確診個案上升，全國累積十四縣市共一三九所校園停課，各縣市對畢業旅行、戶外教學是否取消標準不一，教師團體認為政府應該明確表態，否則會造成校園恐慌或影響學生受教權。陳時中表示，未來必然走向與病毒共存，下周將與教育部檢討停課標準，縮小匡列範圍，並在合理範圍以居家快篩來取代停課。\n",
        "\n",
        "疫情一至兩個月達高峰\n",
        "指揮中心比照韓國、紐西蘭及香港疫情發展，推估未來一至兩個月確診案例將飆至最高峰，陳時中表示，目前Omicron確診數仍算低，但規模難以預估，未來單日恐超過一千五百例，屆時致死率、個案數飆高或疫情高峰下不來，社會將承擔不起，因此仍須積極因應，朝「緩坡上升」方向努力。\n",
        "\n",
        "輕症在家指引至今沒譜\n",
        "總統蔡英文日前宣布防疫以「減災」為目標避免醫療量能超載，「輕症在家」隔離為其中配套，單日確診數若達一千五百人將啟動。指揮中心下周將擬定「輕症在家照護指引」，如設醫療遠距平台、送藥、戶政及警政系統聯繫、關懷中心運作等，若未遵守隔離規定將有罰則，並因應地方疫情升溫和醫療量能吃緊，將從新北試辦。\n",
        "\n",
        "立委賴惠員、蔣萬安昨於衛環委員會質詢時，質疑各地方早已喊話希望指揮中心相關指引快出來，但至今連社區溝通、徵求試辦的地方政府意願統統沒有譜。\n",
        "\n",
        "台北市長柯文哲昨晚也在臉書表示「北市防疫旅館量能告急」，因每天確診人數不斷攀升，近期有近萬名移工入境，幾乎把北市的防疫旅館量能占滿。北市已發出徵召令，徵用加強版防疫專責旅館，讓輕症、低危險確診者入住。\n",
        "\n",
        "總統府、監察院傳確診者\n",
        "本土疫情多點爆發，公務機關包括總統府、監察院、台北市議會都傳出有確診者，北中南八大行業確診人數也驟增，未來場所是否新增禁令，指揮中心將再討論。\n",
        "\n",
        "境外移入昨增一四九例個案，有七十八例為航班落地採檢陽性，越南有十七例居冠。指揮中心表示，目前落地採檢陽性率約落在四％到五％間，即日起越南航空、越捷航空及越竹航空等三航空公司班機，增加「搭機前六小時內抗原快篩報告」才可入境。\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNBQwui9Dwk4"
      },
      "outputs": [],
      "source": [
        "# Start by turning a text into a spaCy Doc object\n",
        "zh_doc = zh(zh_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R7DG4a3Dv9I"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `zh_toks`.===#\n",
        "\n",
        "# zh_toks ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blh1uKC6EEbk"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `zh_ents`.===#\n",
        "\n",
        "# zh_ents ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpyxvj-kXSEZ"
      },
      "source": [
        "- Input 2: Simplified version of Input 1 (Use `opencc` to do the conversion.)\n",
        "- Ouput 2:\n",
        "    - A list of unique tokens except for punctuations\n",
        "    - A list of unique tuples of (NER text, NER label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7nTkl1vS9oz"
      },
      "outputs": [],
      "source": [
        "# @title opencc Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    !pip install opencc -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJH14sEYEs_F"
      },
      "outputs": [],
      "source": [
        "import opencc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9kuUV-wEr_t"
      },
      "outputs": [],
      "source": [
        "converter = opencc.OpenCC('t2s.json')\n",
        "sim_zh_input = converter.convert(zh_input)\n",
        "sim_zh_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIPXxptSFANd"
      },
      "outputs": [],
      "source": [
        "# Start by turning a text into a spaCy Doc object\n",
        "sim_zh_doc = zh(sim_zh_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e9YFa0UE6UO"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `sim_zh_toks`.===#\n",
        "\n",
        "# sim_zh_toks ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb_arUXbG-7L"
      },
      "source": [
        "Evaluate whether `zh_toks` is equal to `sim_zh_toks`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq437wxjFZCX"
      },
      "outputs": [],
      "source": [
        "zh_toks == sim_zh_toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMnHvEMsFM_7"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `sim_zh_ents`.===#\n",
        "\n",
        "sim_zh_ents = set((ent.text, ent.label_) for ent in sim_zh_doc.ents)\n",
        "sim_zh_ents\n",
        "# sim_zh_ents ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2h24d8dHJiD"
      },
      "source": [
        "Evaluate whether `zh_ents` is equal to `sim_zh_ents`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8VTxRwWFjX-"
      },
      "outputs": [],
      "source": [
        "zh_ents == sim_zh_ents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plAw4jVrIECt"
      },
      "source": [
        "## 📚 Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cnjCdphIF_O"
      },
      "source": [
        "1. https://ckip.iis.sinica.edu.tw/\n",
        "2. https://github.com/APCLab/jieba-tw\n",
        "2. https://corenlp.run/\n",
        "3. https://github.com/Embedding/Chinese-Word-Vectors\n",
        "4. https://github.com/stanfordnlp/GloVe\n",
        "5. https://radimrehurek.com/gensim/\n",
        "7. https://github.com/sloria/textblob\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}