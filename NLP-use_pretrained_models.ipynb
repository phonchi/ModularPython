{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuOXkMR73rhs"
      },
      "source": [
        "# NLP: Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/phonchi/ModularPython/blob/master/NLP-use-pretrained-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/phonchi/ModularPython/blob/master/NLP-use-pretrained-models.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gHdfxlxuvo3"
      },
      "source": [
        "This notebook is adapted by [Haowen Jiang](https://howard-haowen.rohan.tw/) from [this one](https://github.com/nlptown/nlp-notebooks/blob/master/NLP%20with%20pretrained%20models%20-%20spaCy%20and%20StanfordNLP.ipynb) included in the [nlptown\n",
        "/nlp-notebooks](https://github.com/nlptown/nlp-notebooks) repo. It is meant for the 2022 [NLP Workshop at NSYSU](https://howard-haowen.github.io/NLP-demos/nsysu_workshop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUnEs1QovufO"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "today = date.today()\n",
        "print(\"Last updated:\", today)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGALGAK2wk-0"
      },
      "source": [
        "# ğŸ“˜ NLP with pretrained models - spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T0bh4HV62Vo"
      },
      "outputs": [],
      "source": [
        "# @title spaCy Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    #!pip install -U pip setuptools wheel -qq\n",
        "    #!pip install -U spacy -qq\n",
        "    !python -m spacy download en_core_web_md -qq # downloads the medium-sized English language model\n",
        "    !python -m spacy download zh_core_web_md -qq # downloads the medium-sized Chinese language model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://spacy.io/images/pipeline.svg)"
      ],
      "metadata": {
        "id": "Pu9hGUkN1toE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG9Fr9tMwk-5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ0etPoz2Xuy"
      },
      "outputs": [],
      "source": [
        "spacy.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUXK0igWCdoq"
      },
      "source": [
        "- To get you started, play with [this Web App](https://share.streamlit.io/howard-haowen/spacy-streamlit/app.py) that I created, which is powered by spaCy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDns3sBg2rvZ"
      },
      "source": [
        "## English NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy_QA0ib7hE1"
      },
      "outputs": [],
      "source": [
        "en = spacy.load(\"en_core_web_md\") # Loading the spaCy Model which includes vocabulary, syntax models, and entities.\n",
        "df_metadata = pd.DataFrame([en.meta])\n",
        "df_metadata.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlVecKvxwk-7"
      },
      "outputs": [],
      "source": [
        "text = (\"Donald John Trump (born June 14, 1946) is the 45th and previous president of \"\n",
        "     \"the United States.  Before entering politics, he was a businessman and television personality.\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQcxTvqj6M8y"
      },
      "source": [
        "Here, the text about Donald Trump is processed by the spaCy model, creating a `Doc` object (A `Doc` object is a sequence of Token objects representing a lexical token) `doc_en` that contains all the information about the text's structure and content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l3k7kszwk-7"
      },
      "outputs": [],
      "source": [
        "doc_en = en(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g_rM0qIwk-8"
      },
      "outputs": [],
      "source": [
        "tokens = [token.text for token in doc_en]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSv5WA3twk-8"
      },
      "source": [
        "SpaCy also splits your document into sentences. In spaCy, the `.sents` property is used to extract sentences from the Doc object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IduCRRqwk-8"
      },
      "outputs": [],
      "source": [
        "sentences = list(doc_en.sents)\n",
        "len(sentences), sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-Speech tagging"
      ],
      "metadata": {
        "id": "8Ox9qEGThLjv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hokEXPh0wk-9"
      },
      "source": [
        "In addition, spaCy identifies a variety of linguistic features for each token. Among the foundational features are the lemma and two types of parts-of-speech (POS) tags. The `pos_` attribute encompasses the [Universal POS tags](https://universaldependencies.org/u/pos/) derived from the [Universal Dependencies](https://universaldependencies.org/) framework, which provide a consistent categorization of word types across languages. On the other hand, the `tag_` attribute offers more detailed, language-specific POS tags that capture finer grammatical distinctions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence."
      ],
      "metadata": {
        "id": "VSh8N2w6ghMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fQ81Ye9wk-_"
      },
      "outputs": [],
      "source": [
        "# orthographic representation, lemma, coarse-grained part-of-speech (pos_), and fine-grained part-of-speech (tag_).\n",
        "features = [\n",
        "    {'Text': token.orth_, 'Lemma': token.lemma_, 'POS': token.pos_, 'Detailed POS': token.tag_, 'Explain': spacy.explain(token.tag_)}\n",
        "    for token in doc_en\n",
        "]\n",
        "\n",
        "df_features = pd.DataFrame(features)\n",
        "df_features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named-Entity Recognition"
      ],
      "metadata": {
        "id": "Al-MsZXMhU7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ZjaXBfwk_A"
      },
      "source": [
        "Next, spaCy includes pre-trained models for named entity recognition (NER). The outcomes of these models are reflected in the `ent_iob_` and `ent_type` attributes. The `ent_type` attribute specifies the category of entity identified by the model, such as a person, date, ordinal number, or geopolitical entity (GPE). For instance, in English models adhering to the [OntoNotes standard](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf), \"Donald John Trump\" is recognized as a person, \"June 14, 1946\" as a date, \"45th\" as an ordinal number, and \"the United States\" as a GPE.\n",
        "\n",
        "The `ent_iob_` attribute (inside-outside-beginning (IOB) tagging) indicates the token's position within an entity: `O` for outside any entity, `B` for the beginning of an entity, and `I` for inside an entity (but not at the beginning). This notation is part of the `BIO` tagging scheme, which helps differentiate between consecutive entities of the same type.\n",
        "\n",
        "> Other schemes like `BILUO` include additional designations for the last token of an entity and for unique, standalone entity tokens, providing detailed positional information within entity sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-es444kwk_A"
      },
      "outputs": [],
      "source": [
        "# Extracting named entity information from each token in the document\n",
        "entities = [\n",
        "    {'Text': token.orth_, 'IOB Tag': token.ent_iob_, 'Entity Type': token.ent_type_, 'Explain': spacy.explain(token.ent_type_)} #_ is to get the string\n",
        "    for token in doc_en  # Iterate over each token\n",
        "]\n",
        "\n",
        "df_entities = pd.DataFrame(entities)\n",
        "df_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAnHlup7wk_B"
      },
      "source": [
        "You can also access the entities directly on the `ents` attribute of the document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eguxaqnVwk_B"
      },
      "outputs": [],
      "source": [
        "print([(ent.text, ent.label_) for ent in doc_en.ents])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Parsing"
      ],
      "metadata": {
        "id": "YLTGlUldjYZm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_lp-ih4wk_B"
      },
      "source": [
        "spaCy also contains a dependency parser, which analyzes the grammatical relations between the tokens.\n",
        "\n",
        "> Dependency parsing is the process of extracting the dependency graph of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the root of the sentence. All other words are linked to the headword. Specifically,the dependencies can be mapped in a directed graph representation where words are the nodes and grammatical relationships are the edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZHHNKwTwk_C"
      },
      "outputs": [],
      "source": [
        "# Extracting syntax or dependency parsing information from each token\n",
        "syntax = [\n",
        "    {'Token': token.text, 'Dependency': token.dep_, 'Head': token.head.text, 'Explain': spacy.explain(token.dep_)}\n",
        "    for token in doc_en  # Iterate over each token in the document\n",
        "]\n",
        "\n",
        "df_syntax = pd.DataFrame(syntax)\n",
        "df_syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umAUHhTdQU8b"
      },
      "source": [
        "Finally, the English spaCy model contains a morphological parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpFmyJaVQVm2"
      },
      "outputs": [],
      "source": [
        "# Extracting morphological features from each token in the document\n",
        "features = [\n",
        "    {'Token': token.text, 'Morphological Features': token.morph}\n",
        "    for token in doc_en  # Iterate over each token\n",
        "]\n",
        "\n",
        "df_features = pd.DataFrame(features)\n",
        "df_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLClbFviwk_C"
      },
      "source": [
        "## Multilingual NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbp79ilwk_C"
      },
      "source": [
        "SpaCy doesn't only have models for English, but also for many other languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5zp8GAz8hxh"
      },
      "outputs": [],
      "source": [
        "zh = spacy.load(\"zh_core_web_md\")\n",
        "df_metadata = pd.DataFrame([en.meta])\n",
        "df_metadata.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HemfWhawk_C"
      },
      "outputs": [],
      "source": [
        "text_zh = \"ä¸­å±±å¤§å­¸äººæ–‡æš¨ç§‘æŠ€è·¨é ˜åŸŸå­¸å£«å­¸ä½å­¸ç¨‹åŠ©ç†æ•™æˆå®‹ä¸–ç¥¥è¡¨ç¤ºï¼Œ2021å¹´è–èª•ç¯€å‡æœŸæœŸé–“ï¼Œå¸«ç”Ÿèˆ‰è¾¦ã€Œè¡—é ­ç©ç«¥ï½é¹½åŸ•å…’ç«¥è¡—å€éŠæˆ²æ—¥ã€æˆæœå±•ã€‚æ´»å‹•ä¸­å¯çœ‹è¦‹å­¸ç”Ÿé‹ç”¨è´ŠåŠ©å–®ä½ç‘å„€æ•™è‚²åŸºé‡‘æœƒè‡´è´ˆçš„å»¢æ£„æœ¨æ£§æ¿ï¼Œè£½ä½œäº†6å…·å…’ç«¥å‰µæ„éŠå…·ï¼Œä¸€æ–¹é¢å±•ç¤ºå­¸ç¿’æˆæœï¼Œä¹Ÿå¸Œæœ›è—‰æ­¤å‘¼ç±²é«˜é›„æ°‘çœ¾é‡è¦–å…’ç«¥çš„éŠæˆ²æ¬Šã€‚\"\n",
        "doc_zh = zh(text_zh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsF4wHjwwk_D"
      },
      "source": [
        "The tokens in the Chinese document share the same attribute structure as those in the English document in spaCy. However, the functionalities of the models can vary significantly between languages. One key difference to note is in the handling of lemmatization:\n",
        "\n",
        "- **Lack of Lemmatization in Chinese Model**: Unlike the English model, the Chinese model does not provide lemmatization.\n",
        "\n",
        "This distinction is important to consider when performing text processing tasks, as it affects the depth of linguistic analysis available for each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AogpQPsO8DRR"
      },
      "outputs": [],
      "source": [
        "list(doc_zh.sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7uMi5669USU"
      },
      "outputs": [],
      "source": [
        "tok_text = [tok.text for tok in doc_zh]\n",
        "tok_orth = [tok.orth_ for tok in doc_zh]\n",
        "print(tok_text)\n",
        "print(tok_orth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zf3TSWY95hc"
      },
      "outputs": [],
      "source": [
        "for tok in list(doc_zh.sents)[1]: # The second sentence\n",
        "    print(f\"{tok.text} >>> {tok.pos_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8SN7SUAX8y"
      },
      "source": [
        "- The Chinese model has a very different fine-grained part-of-speech tags on the `tag_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgpNWCl4AAke"
      },
      "outputs": [],
      "source": [
        "# Printing each token's text, detailed POS tag, and an explanation of the tag.\n",
        "for tok in list(doc_zh.sents)[1]:\n",
        "    print(f\"{tok.text} >>> {tok.tag_} | {spacy.explain(tok.tag_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZmsq_gYAc0T"
      },
      "source": [
        "- The Chinese model has different entity types (PER, LOC and ORG) than the English one.\n",
        "\n",
        "This is a result of the training corpora that were used to build the models, whose annotation guidelines may be very different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3hIsara5th9"
      },
      "outputs": [],
      "source": [
        "info = [(t.text, t.pos_, t.tag_, t.ent_iob_, t.ent_type_) for t in doc_zh]\n",
        "df_info = pd.DataFrame(info, columns=['Text', 'POS', 'Tag', 'IOB Tag', 'Entity Type'])\n",
        "df_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GyNQ4UjDqTH"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoENmU8oBQMt"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNGB9n6nDuTE"
      },
      "outputs": [],
      "source": [
        "displacy.render(doc_zh, style='ent',jupyter=True, options={'distance':130})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bZWBpW8BR73"
      },
      "outputs": [],
      "source": [
        "text = \"æˆ‘æƒ³è¦ä¸‰ä»½2è™Ÿé¤\"\n",
        "doc = zh(text)\n",
        "displacy.render(doc, style='dep',jupyter=True, options={'distance':130})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-A6lmiG_1Yd"
      },
      "source": [
        "## DataFrame + spaCy = dframcy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQBGx3gB-sQK"
      },
      "outputs": [],
      "source": [
        "# @title dframcy Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    !pip install dframcy -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0sexga0-yWU"
      },
      "outputs": [],
      "source": [
        "from dframcy import DframCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDO97xhS-pnj"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('zh_core_web_md')\n",
        "# Initialize DframCy with the spaCy NLP model to integrate with pandas DataFrame.\n",
        "dframcy = DframCy(nlp)\n",
        "# Process the Chinese text using the NLP model to create a spaCy document.\n",
        "doc = dframcy.nlp(text_zh)\n",
        "# Convert the NLP document annotations to a pandas DataFrame for easier analysis.\n",
        "annotation_dataframe = dframcy.to_dataframe(doc)\n",
        "annotation_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7u0msqWA2Km"
      },
      "source": [
        "Once annotations are stored as a DataFrame object, filtering can be easily done by leveraging the power of `pandas` syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK2eJJojEf_I"
      },
      "outputs": [],
      "source": [
        "# Create a filter for rows where the part-of-speech tag is 'NN' (noun).\n",
        "nn_filt = annotation_dataframe['token_tag_'] == 'NN'\n",
        "# Create a filter for rows where the dependency label is 'dobj' (direct object).\n",
        "dobj_filt = annotation_dataframe['token_dep_'] == 'dobj'\n",
        "# Get rows where the token is a noun and serves as a direct object.\n",
        "annotation_dataframe[(nn_filt) & dobj_filt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCpCdxLhF0Pq"
      },
      "source": [
        "## Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2120c0njF2T1"
      },
      "outputs": [],
      "source": [
        "doc = zh(\"æ•™æˆ\")\n",
        "tok = doc[0]\n",
        "tok.vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX_NjkK1F-mP"
      },
      "outputs": [],
      "source": [
        "tok.vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    # Ensure the vectors are not only zeros\n",
        "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
        "        return 0.0\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "\n",
        "# Define the words and retrieve vectors\n",
        "word_1 = zh(\"é«˜èˆˆ\").vector\n",
        "word_2 = zh(\"é«˜é›„\").vector\n",
        "word_3 = zh(\"é–‹å¿ƒ\").vector\n",
        "\n",
        "# Calculate similarities\n",
        "word_1_word_2_similarity = cosine_similarity(word_1, word_2)\n",
        "word_1_word_3_similarity = cosine_similarity(word_1, word_3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Distance between 'é«˜èˆˆ' and 'é«˜é›„': {word_1_word_2_similarity}\")\n",
        "print(f\"Distance between 'é«˜èˆˆ' and 'é–‹å¿ƒ': {word_1_word_3_similarity}\")"
      ],
      "metadata": {
        "id": "z8Rvx7YmA5HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTJbrvaAGNvq"
      },
      "source": [
        "- Cosine similarity\n",
        "\n",
        "![](https://zhangruochi.com/Operations-on-word-vectors-Debiasing/2019/03/28/images/cosine_sim.png)\n",
        "\n",
        "- Formula for calculating cosine similarity between two vectors\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*LfW66-WsYkFqWc4XYJbEJg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04olEHqfwk_D"
      },
      "source": [
        "## ğŸ” Supplementary: StanfordNLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghzefh-xwk_D"
      },
      "source": [
        "Another library that shares some functionality with spaCy is StanfordNLP. [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/), distinct from Stanfordâ€™s Java-based [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) library, is a [Python library](https://github.com/stanfordnlp/stanfordnlp) developed on the PyTorch framework. It provides a fully neural NLP pipeline, which includes advanced features such as tokenization (capable of recognizing multi-word units), lemmatization, part-of-speech tagging (incorporating morphological features), and state-of-the-art dependency parsing. These components were specifically designed and trained for the [CoNLL-2018 shared task](https://nlp.stanford.edu/pubs/qi2018universal.pdf). While it does not include named entity recognition, StanfordNLP excels in dependency parsing and additionally offers a Python interface to CoreNLP, facilitating integration into Python projects.\n",
        "\n",
        "This version provides a clearer distinction between the two Stanford libraries and emphasizes the specific capabilities and strengths of StanfordNLP, enhancing the reader's understanding of its purpose and utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9UtQCFzwk_D"
      },
      "source": [
        "> **`stanfordnlp` has been renamed to `stanza`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZncAMak2G-0S"
      },
      "outputs": [],
      "source": [
        "# @title stanza Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    !pip install stanza -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ct8iirDHhUa"
      },
      "outputs": [],
      "source": [
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7S29T-ZKjWg"
      },
      "outputs": [],
      "source": [
        "stanza.download(\"zh-hant\") # Download the traditional Chinese model for Stanza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5NFJi_WMByF"
      },
      "outputs": [],
      "source": [
        "stf_nlp = stanza.Pipeline('zh-hant') # Initialize the Stanza pipeline for traditional Chinese to handle various NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmZRFfdYHSnL"
      },
      "outputs": [],
      "source": [
        "text_zh = \"ä¸­å±±å¤§å­¸äººæ–‡æš¨ç§‘æŠ€è·¨é ˜åŸŸå­¸å£«å­¸ä½å­¸ç¨‹åŠ©ç†æ•™æˆå®‹ä¸–ç¥¥è¡¨ç¤ºï¼Œ2021å¹´è–èª•ç¯€å‡æœŸæœŸé–“ï¼Œå¸«ç”Ÿèˆ‰è¾¦ã€Œè¡—é ­ç©ç«¥ï½é¹½åŸ•å…’ç«¥è¡—å€éŠæˆ²æ—¥ã€æˆæœå±•ã€‚æ´»å‹•ä¸­å¯çœ‹è¦‹å­¸ç”Ÿé‹ç”¨è´ŠåŠ©å–®ä½ç‘å„€æ•™è‚²åŸºé‡‘æœƒè‡´è´ˆçš„å»¢æ£„æœ¨æ£§æ¿ï¼Œè£½ä½œäº†6å…·å…’ç«¥å‰µæ„éŠå…·ï¼Œä¸€æ–¹é¢å±•ç¤ºå­¸ç¿’æˆæœï¼Œä¹Ÿå¸Œæœ›è—‰æ­¤å‘¼ç±²é«˜é›„æ°‘çœ¾é‡è¦–å…’ç«¥çš„éŠæˆ²æ¬Šã€‚\"\n",
        "# Process the text with the Stanza pipeline to extract linguistic information.\n",
        "doc = stf_nlp(text_zh)\n",
        "type(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXC7KjTsVFQ6"
      },
      "source": [
        "Different models often produce different tokenization results, which in turn would have impact on POS and DEP tagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kci7Uko7VZcg"
      },
      "source": [
        "- Here're the results based on StandfordNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENBx-6QY74qU"
      },
      "outputs": [],
      "source": [
        "words_data = []\n",
        "for i, sent in enumerate(doc.sentences):\n",
        "    for word in sent.words:\n",
        "        # Prepare and append a dictionary with details about each word to the list.\n",
        "        words_data.append({\n",
        "            'Sentence Number': i + 1,\n",
        "            'Text': word.text,\n",
        "            'Lemma': word.lemma,\n",
        "            'POS': word.pos,\n",
        "            'Head Index': word.head,\n",
        "            'Dependency Relation': word.deprel\n",
        "        })\n",
        "\n",
        "df_words = pd.DataFrame(words_data)\n",
        "df_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE5T8kxWSwmH"
      },
      "source": [
        "## ğŸ” Supplementary: Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfDmTaw0TrZ7"
      },
      "source": [
        "### Analyze Enlgish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFjZct1ES--5"
      },
      "source": [
        "- Input: any English news article of your choice\n",
        "- Ouput:\n",
        "    - A list of unique lemmas of all verbs in lower case\n",
        "    - A list of unique tuples of (NER text, NER label)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWg-kuyPXagy"
      },
      "outputs": [],
      "source": [
        "# Change this to any other article of your choice.\n",
        "\n",
        "en_input = \"\"\"\n",
        "Taipei, April 7 (CNA) Health and Welfare Minister Chen Shih-chung (é™³æ™‚ä¸­) said Thursday that COVID-19 contact tracing has been partially suspended in Taiwan and a new disease control model is being put in place, amid a rise in domestic cases.\n",
        "\n",
        "The immediate suspension of contract tracing applies only to travelers who test positive for COVID-19 in Taiwan, either on arrival at the airport or during mandatory quarantine, Chen said.\n",
        "\n",
        "That decision was made in a bid to free up resources to monitor the growing number of domestic COVID-19 cases, he said at a press briefing, after he reported 531 new cases -- 382 domestically transmitted and 149 imported.\n",
        "\n",
        "Chen said contact tracing on new imported cases will only be done if any of them are believed to be linked to COVID-19 clusters at quarantine hotels or quarantine centers in Taiwan.\n",
        "\n",
        "Prior to Thursday, Taiwan had been reporting its contact tracing information on imported COVID-19 cases via the World Health Organization's International Health Regulations (IHR) mechanism, he said.\n",
        "\n",
        "Regarding the recent daily rise in domestic infections, Chen said the current goal is to bring the situation under control, even though it is impossible to achieve zero new domestic cases at this time.\n",
        "\n",
        "Despite the recent spike, the daily number of domestic COVID-19 cases in Taiwan is still low compared to many other countries, he said, citing as an example the 534 new cases per 100,000 population reported in South Korea on Tuesday.\n",
        "\n",
        "Once people in Taiwan stick together and do their part to prevent the spread of the virus, the situation will be manageable, Chen said.\n",
        "\n",
        "Based on the trajectory of COVID-19 Omicron outbreaks observed in many other countries around the world, he said, it is likely that the infections in Taiwan will peak in a month or two.\n",
        "\n",
        "\"We do not expect the outbreak to stop growing now, but we hope it will rise slowly, so that Taiwan's medical capacity will not be overloaded,\" Chen said.\n",
        "\n",
        "Meanwhile, earlier in the day, the Cabinet announced that Taiwan was adopting a new model for the control of COVID-19 infections.\n",
        "\n",
        "Under the \"new Taiwan model,\" the country has let go of its goal to achieve zero COVID-19 cases, but this does not mean allowing the pandemic go unmanaged, Cabinet spokesman Lo Ping-cheng (ç¾…ç§‰æˆ) said, citing Premier Su Tseng-chang (è˜‡è²æ˜Œ).\n",
        "\n",
        "In a meeting earlier with Ministry of Health and Welfare (MOHW) officials, Premier Su said that as Taiwan moves towards a new stage of epidemic prevention, he hopes that the central and local governments will work together to gradually open up the country, in the interests of its people and economy, according to Lo.\n",
        "\n",
        "In a report presented to the Cabinet on Thursday, the MOHW said Taiwan will continue to actively manage the COVID-19 situation, while steadily opening up its borders, in consideration of national economic factors and the people's livelihood.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fsb37mkWDTlR"
      },
      "outputs": [],
      "source": [
        "# Start by turning a text into a spaCy Doc object\n",
        "en_doc = en(en_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHBSsFhDHahi"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `verbs`.===#\n",
        "\n",
        "\n",
        "# verbs ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_cLlJhODK-K"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `en_ents`.===#\n",
        "\n",
        "# en_ents ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54HIkSkRTzqe"
      },
      "source": [
        "### Analyze Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaHx9Pj-VpPj"
      },
      "source": [
        "- Input 1: any Chinese news article from Taiwan media of your choice\n",
        "- Ouput 1:\n",
        "    - A list of unique tokens except for punctuations\n",
        "    - A list of unique tuples of (NER text, NER label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHfzWXqpXrsI"
      },
      "outputs": [],
      "source": [
        "# Change this to any other article of your choice.\n",
        "\n",
        "zh_input = \"\"\"\n",
        "æœ¬åœŸå€‹æ¡ˆæ˜¨å¢ä¸‰å…«äºŒä¾‹å†å‰µæ–°é«˜ï¼Œç¢ºè¨ºæ¡ˆä¾‹éåŠåä¹ç¸£å¸‚ï¼Œå¢ƒå¤–å¢ä¸€å››ä¹ä¾‹ï¼Œå–®æ—¥ç ´äº”ç™¾æ¡ˆä¾‹ã€‚\n",
        "\n",
        "ä¸­å¤®ç–«æƒ…æŒ‡æ®ä¸­å¿ƒæŒ‡æ®å®˜é™³æ™‚ä¸­è¡¨ç¤ºï¼Œå€‹æ¡ˆæ•¸é‚„æœƒå¾€ä¸Šå‡ï¼Œé ä¼°ä¸€è‡³å…©å€‹æœˆå…§é”æœ€é«˜å³°ï¼Œç–«æƒ…å°‡æŒçºŒåˆ°å…­æœˆåº•ä¸”é‚„ä¸æœƒåˆ°å°¾è²ï¼›ä»–ä¹Ÿé¦–åº¦é¬†å£ã€Œæ¸…é›¶ä¸å¯èƒ½ã€ï¼Œæœªä¾†å°‡èµ°å‘èˆ‡ç—…æ¯’å…±å­˜ï¼Œä¸‹å‘¨æ“¬è©¦è¾¦ã€Œè¼•ç—‡åœ¨å®¶ã€éš”é›¢ï¼ŒåŒæ™‚ä¹Ÿå°‡èª¿æ•´åœèª²æ¨™æº–ã€‚\n",
        "\n",
        "æ“¬ä»¥å±…å®¶å¿«ç¯©å–ä»£åœèª²\n",
        "åœ‹å…§ç¢ºè¨ºå€‹æ¡ˆä¸Šå‡ï¼Œå…¨åœ‹ç´¯ç©åå››ç¸£å¸‚å…±ä¸€ä¸‰ä¹æ‰€æ ¡åœ’åœèª²ï¼Œå„ç¸£å¸‚å°ç•¢æ¥­æ—…è¡Œã€æˆ¶å¤–æ•™å­¸æ˜¯å¦å–æ¶ˆæ¨™æº–ä¸ä¸€ï¼Œæ•™å¸«åœ˜é«”èªç‚ºæ”¿åºœæ‡‰è©²æ˜ç¢ºè¡¨æ…‹ï¼Œå¦å‰‡æœƒé€ æˆæ ¡åœ’ææ…Œæˆ–å½±éŸ¿å­¸ç”Ÿå—æ•™æ¬Šã€‚é™³æ™‚ä¸­è¡¨ç¤ºï¼Œæœªä¾†å¿…ç„¶èµ°å‘èˆ‡ç—…æ¯’å…±å­˜ï¼Œä¸‹å‘¨å°‡èˆ‡æ•™è‚²éƒ¨æª¢è¨åœèª²æ¨™æº–ï¼Œç¸®å°åŒ¡åˆ—ç¯„åœï¼Œä¸¦åœ¨åˆç†ç¯„åœä»¥å±…å®¶å¿«ç¯©ä¾†å–ä»£åœèª²ã€‚\n",
        "\n",
        "ç–«æƒ…ä¸€è‡³å…©å€‹æœˆé”é«˜å³°\n",
        "æŒ‡æ®ä¸­å¿ƒæ¯”ç…§éŸ“åœ‹ã€ç´è¥¿è˜­åŠé¦™æ¸¯ç–«æƒ…ç™¼å±•ï¼Œæ¨ä¼°æœªä¾†ä¸€è‡³å…©å€‹æœˆç¢ºè¨ºæ¡ˆä¾‹å°‡é£†è‡³æœ€é«˜å³°ï¼Œé™³æ™‚ä¸­è¡¨ç¤ºï¼Œç›®å‰Omicronç¢ºè¨ºæ•¸ä»ç®—ä½ï¼Œä½†è¦æ¨¡é›£ä»¥é ä¼°ï¼Œæœªä¾†å–®æ—¥æè¶…éä¸€åƒäº”ç™¾ä¾‹ï¼Œå±†æ™‚è‡´æ­»ç‡ã€å€‹æ¡ˆæ•¸é£†é«˜æˆ–ç–«æƒ…é«˜å³°ä¸‹ä¸ä¾†ï¼Œç¤¾æœƒå°‡æ‰¿æ“”ä¸èµ·ï¼Œå› æ­¤ä»é ˆç©æ¥µå› æ‡‰ï¼Œæœã€Œç·©å¡ä¸Šå‡ã€æ–¹å‘åŠªåŠ›ã€‚\n",
        "\n",
        "è¼•ç—‡åœ¨å®¶æŒ‡å¼•è‡³ä»Šæ²’è­œ\n",
        "ç¸½çµ±è”¡è‹±æ–‡æ—¥å‰å®£å¸ƒé˜²ç–«ä»¥ã€Œæ¸›ç½ã€ç‚ºç›®æ¨™é¿å…é†«ç™‚é‡èƒ½è¶…è¼‰ï¼Œã€Œè¼•ç—‡åœ¨å®¶ã€éš”é›¢ç‚ºå…¶ä¸­é…å¥—ï¼Œå–®æ—¥ç¢ºè¨ºæ•¸è‹¥é”ä¸€åƒäº”ç™¾äººå°‡å•Ÿå‹•ã€‚æŒ‡æ®ä¸­å¿ƒä¸‹å‘¨å°‡æ“¬å®šã€Œè¼•ç—‡åœ¨å®¶ç…§è­·æŒ‡å¼•ã€ï¼Œå¦‚è¨­é†«ç™‚é è·å¹³å°ã€é€è—¥ã€æˆ¶æ”¿åŠè­¦æ”¿ç³»çµ±è¯ç¹«ã€é—œæ‡·ä¸­å¿ƒé‹ä½œç­‰ï¼Œè‹¥æœªéµå®ˆéš”é›¢è¦å®šå°‡æœ‰ç½°å‰‡ï¼Œä¸¦å› æ‡‰åœ°æ–¹ç–«æƒ…å‡æº«å’Œé†«ç™‚é‡èƒ½åƒç·Šï¼Œå°‡å¾æ–°åŒ—è©¦è¾¦ã€‚\n",
        "\n",
        "ç«‹å§”è³´æƒ å“¡ã€è”£è¬å®‰æ˜¨æ–¼è¡›ç’°å§”å“¡æœƒè³ªè©¢æ™‚ï¼Œè³ªç–‘å„åœ°æ–¹æ—©å·²å–Šè©±å¸Œæœ›æŒ‡æ®ä¸­å¿ƒç›¸é—œæŒ‡å¼•å¿«å‡ºä¾†ï¼Œä½†è‡³ä»Šé€£ç¤¾å€æºé€šã€å¾µæ±‚è©¦è¾¦çš„åœ°æ–¹æ”¿åºœæ„é¡˜çµ±çµ±æ²’æœ‰è­œã€‚\n",
        "\n",
        "å°åŒ—å¸‚é•·æŸ¯æ–‡å“²æ˜¨æ™šä¹Ÿåœ¨è‡‰æ›¸è¡¨ç¤ºã€ŒåŒ—å¸‚é˜²ç–«æ—…é¤¨é‡èƒ½å‘Šæ€¥ã€ï¼Œå› æ¯å¤©ç¢ºè¨ºäººæ•¸ä¸æ–·æ”€å‡ï¼Œè¿‘æœŸæœ‰è¿‘è¬åç§»å·¥å…¥å¢ƒï¼Œå¹¾ä¹æŠŠåŒ—å¸‚çš„é˜²ç–«æ—…é¤¨é‡èƒ½å æ»¿ã€‚åŒ—å¸‚å·²ç™¼å‡ºå¾µå¬ä»¤ï¼Œå¾µç”¨åŠ å¼·ç‰ˆé˜²ç–«å°ˆè²¬æ—…é¤¨ï¼Œè®“è¼•ç—‡ã€ä½å±éšªç¢ºè¨ºè€…å…¥ä½ã€‚\n",
        "\n",
        "ç¸½çµ±åºœã€ç›£å¯Ÿé™¢å‚³ç¢ºè¨ºè€…\n",
        "æœ¬åœŸç–«æƒ…å¤šé»çˆ†ç™¼ï¼Œå…¬å‹™æ©Ÿé—œåŒ…æ‹¬ç¸½çµ±åºœã€ç›£å¯Ÿé™¢ã€å°åŒ—å¸‚è­°æœƒéƒ½å‚³å‡ºæœ‰ç¢ºè¨ºè€…ï¼ŒåŒ—ä¸­å—å…«å¤§è¡Œæ¥­ç¢ºè¨ºäººæ•¸ä¹Ÿé©Ÿå¢ï¼Œæœªä¾†å ´æ‰€æ˜¯å¦æ–°å¢ç¦ä»¤ï¼ŒæŒ‡æ®ä¸­å¿ƒå°‡å†è¨è«–ã€‚\n",
        "\n",
        "å¢ƒå¤–ç§»å…¥æ˜¨å¢ä¸€å››ä¹ä¾‹å€‹æ¡ˆï¼Œæœ‰ä¸ƒåå…«ä¾‹ç‚ºèˆªç­è½åœ°æ¡æª¢é™½æ€§ï¼Œè¶Šå—æœ‰åä¸ƒä¾‹å±…å† ã€‚æŒ‡æ®ä¸­å¿ƒè¡¨ç¤ºï¼Œç›®å‰è½åœ°æ¡æª¢é™½æ€§ç‡ç´„è½åœ¨å››ï¼…åˆ°äº”ï¼…é–“ï¼Œå³æ—¥èµ·è¶Šå—èˆªç©ºã€è¶Šæ·èˆªç©ºåŠè¶Šç«¹èˆªç©ºç­‰ä¸‰èˆªç©ºå…¬å¸ç­æ©Ÿï¼Œå¢åŠ ã€Œæ­æ©Ÿå‰å…­å°æ™‚å…§æŠ—åŸå¿«ç¯©å ±å‘Šã€æ‰å¯å…¥å¢ƒã€‚\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNBQwui9Dwk4"
      },
      "outputs": [],
      "source": [
        "# Start by turning a text into a spaCy Doc object\n",
        "zh_doc = zh(zh_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R7DG4a3Dv9I"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `zh_toks`.===#\n",
        "\n",
        "# zh_toks ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blh1uKC6EEbk"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `zh_ents`.===#\n",
        "\n",
        "# zh_ents ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpyxvj-kXSEZ"
      },
      "source": [
        "- Input 2: Simplified version of Input 1 (Use `opencc` to do the conversion.)\n",
        "- Ouput 2:\n",
        "    - A list of unique tokens except for punctuations\n",
        "    - A list of unique tuples of (NER text, NER label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7nTkl1vS9oz"
      },
      "outputs": [],
      "source": [
        "# @title opencc Installation { display-mode: \"form\" }\n",
        "\n",
        "INSTALL = True # @param {type:\"boolean\"}\n",
        "\n",
        "if INSTALL:\n",
        "    !pip install opencc -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJH14sEYEs_F"
      },
      "outputs": [],
      "source": [
        "import opencc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9kuUV-wEr_t"
      },
      "outputs": [],
      "source": [
        "converter = opencc.OpenCC('t2s.json')\n",
        "sim_zh_input = converter.convert(zh_input)\n",
        "sim_zh_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIPXxptSFANd"
      },
      "outputs": [],
      "source": [
        "# Start by turning a text into a spaCy Doc object\n",
        "sim_zh_doc = zh(sim_zh_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e9YFa0UE6UO"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `sim_zh_toks`.===#\n",
        "\n",
        "# sim_zh_toks ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb_arUXbG-7L"
      },
      "source": [
        "Evaluate whether `zh_toks` is equal to `sim_zh_toks`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq437wxjFZCX"
      },
      "outputs": [],
      "source": [
        "zh_toks == sim_zh_toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMnHvEMsFM_7"
      },
      "outputs": [],
      "source": [
        "#===Write your code below and save the output as `sim_zh_ents`.===#\n",
        "\n",
        "sim_zh_ents = set((ent.text, ent.label_) for ent in sim_zh_doc.ents)\n",
        "sim_zh_ents\n",
        "# sim_zh_ents ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2h24d8dHJiD"
      },
      "source": [
        "Evaluate whether `zh_ents` is equal to `sim_zh_ents`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8VTxRwWFjX-"
      },
      "outputs": [],
      "source": [
        "zh_ents == sim_zh_ents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plAw4jVrIECt"
      },
      "source": [
        "## ğŸ“š Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cnjCdphIF_O"
      },
      "source": [
        "1. https://ckip.iis.sinica.edu.tw/\n",
        "2. https://github.com/APCLab/jieba-tw\n",
        "2. https://corenlp.run/\n",
        "3. https://github.com/Embedding/Chinese-Word-Vectors\n",
        "4. https://github.com/stanfordnlp/GloVe\n",
        "5. https://radimrehurek.com/gensim/\n",
        "7. https://github.com/sloria/textblob\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}