{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0890095f",
      "metadata": {
        "id": "0890095f"
      },
      "source": [
        "# NLP: scikit-learn basics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WpS1ViW-hSfy",
      "metadata": {
        "id": "WpS1ViW-hSfy"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/phonchi/ModularPython/blob/master/NLP-scikit_learn_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/phonchi/ModularPython/blob/master/NLP-scikit_learn_basics.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21774cfc",
      "metadata": {
        "id": "21774cfc"
      },
      "source": [
        "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
        "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f18cf5",
      "metadata": {
        "id": "42f18cf5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3b8158",
      "metadata": {
        "id": "df3b8158"
      },
      "source": [
        "## Scikit-learn API"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizing Scikit-learn for Machine Learning Models"
      ],
      "metadata": {
        "id": "AkFgTS70362m"
      },
      "id": "AkFgTS70362m"
    },
    {
      "cell_type": "markdown",
      "id": "023903b6",
      "metadata": {
        "id": "023903b6"
      },
      "source": [
        "The `scikit-learn` (or `sklearn`) library is a powerful tool for machine learning, offering a wide range of ready-to-use models and datasets. The algorithms are implemented in a straightforward manner with minimal heuristic modifications, making it ideal for educational purposes. One of the key strengths of `scikit-learn` is its consistent API, which simplifies the process of using different models. The typical workflow for utilizing a model in `scikit-learn` can be summarized as follows:\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA  # Import the model\n",
        "model = PCA(2)              # Initialize the model with hyperparameters\n",
        "X_new = model.fit_transform(X)      # Fit the model to the data and apply transformation\n",
        "```\n",
        "\n",
        "**Terminologies**:\n",
        "- **Data:** A collection of information that may be numerical, textual, visual, or auditory.\n",
        "- **Label:** The target output for each data sample, such as a product review score.\n",
        "- **Model:** A mathematical function used for making predictions or transformations, derived from applying an algorithm to data.\n",
        "- **Fit or Train:** The process of adjusting the model's parameters based on the data.\n",
        "- **Transformation:** The conversion of data into a new format or structure using the model.\n",
        "- **Prediction:** Estimating outputs based on input data using the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c5d9d4",
      "metadata": {
        "id": "a8c5d9d4"
      },
      "source": [
        "We will use the following data to demonstrate two algorithms, PCA and $k$-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e49215",
      "metadata": {
        "id": "31e49215"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('inaugural')\n",
        "\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "files = inaugural.fileids()\n",
        "texts = [inaugural.raw(file) for file in files]\n",
        "years = [file[:-4].split(\"-\")[0] for file in files]\n",
        "presidents = [file[:-4].split(\"-\")[1] for file in files]\n",
        "df = pd.DataFrame({\n",
        "    \"year\": years,\n",
        "    \"president\": presidents,\n",
        "    \"file\": files,\n",
        "    \"text\": texts\n",
        "})\n",
        "df.set_index(\"year\", inplace=True)\n",
        "df.tail() # print last few files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e2fe26",
      "metadata": {
        "id": "98e2fe26"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "X = tfidf.fit_transform(texts).toarray()\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa928790",
      "metadata": {
        "id": "aa928790"
      },
      "source": [
        "### Principal component analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd31e3f",
      "metadata": {
        "id": "1dd31e3f"
      },
      "source": [
        "In our data, each sample is a vector of 8,984 entries.  We usually consider it as a point in an 8,984-dimensional space.  However, it is almost impossible for us to see such a high-dimensional space.  We will transform the data into lower dimension, with the minimum loss of information.  \n",
        "\n",
        "PCA is a dimensionality reduction algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63aa18c2",
      "metadata": {
        "id": "63aa18c2"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "model = PCA(n_components=10)\n",
        "X_new = model.fit_transform(X)\n",
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "exp_var_pca = model.explained_variance_ratio_\n",
        "#\n",
        "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
        "# for visualizing the variance explained by each principal component.\n",
        "#\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "#\n",
        "# Create the visualization plot\n",
        "#\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hps2xS1cxYVk"
      },
      "id": "hps2xS1cxYVk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f45970",
      "metadata": {
        "id": "a2f45970"
      },
      "outputs": [],
      "source": [
        "df['x0'] = X_new[:,0]\n",
        "df['x1'] = X_new[:,1]\n",
        "df.plot(kind='scatter', x='x0', y='x1',\n",
        "    color=df.index.astype(int), hover_data=['president'],\n",
        "    backend='plotly')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0435c3c2",
      "metadata": {
        "id": "0435c3c2"
      },
      "source": [
        "### $k$-means clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e584c2c",
      "metadata": {
        "id": "9e584c2c"
      },
      "source": [
        "When a dataset is given, one may try to partition the data points into several clusters and predict their labels.\n",
        "\n",
        "The $k$-means clustering algorithm will give a \"reasonable\" clustering label to each point."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b831fc",
      "metadata": {
        "id": "20b831fc"
      },
      "source": [
        "_The code below use the same data as above and use the PCA-transformed feature to plot the points._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98eae9e6",
      "metadata": {
        "id": "98eae9e6"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters=3, n_init='auto')\n",
        "y = model.fit_predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffbc52a",
      "metadata": {
        "id": "6ffbc52a"
      },
      "outputs": [],
      "source": [
        "df['y'] = y.astype('object')\n",
        "df.plot(kind='scatter', x='x0', y='x1',\n",
        "        color='y', hover_data=[df.index, 'president'],\n",
        "        backend='plotly')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ” Supplementary: `scikit-ollama`"
      ],
      "metadata": {
        "id": "BSTY8DTbO0ZQ"
      },
      "id": "BSTY8DTbO0ZQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It allows as to seamlessly integrate powerful language models like ChatGPT into scikit-learn for enhanced text analysis tasks."
      ],
      "metadata": {
        "id": "DFNlfVJUPB8v"
      },
      "id": "DFNlfVJUPB8v"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-ollama -qq\n",
        "!pip install colab-xterm -qq\n",
        "!apt install lshw\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "446L5JhfGZjV"
      },
      "id": "446L5JhfGZjV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste the following commands to the terminal:\n",
        "\n",
        "```\n",
        "export OLLAMA_MAX_LOADED_MODELS=2 # sets the max number of loaded models\n",
        "export OLLAMA_NUM_PARALLEL=2 # sets the max number of parallel tasks\n",
        "ollama serve\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "EDS9a9diM9Ew"
      },
      "id": "EDS9a9diM9Ew"
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "-U0fUpG1M2_5"
      },
      "id": "-U0fUpG1M2_5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm height=500"
      ],
      "metadata": {
        "id": "0BkyYywoNWzA"
      },
      "id": "0BkyYywoNWzA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!ollama pull llama3\n",
        "!ollama pull nomic-embed-text"
      ],
      "metadata": {
        "id": "U0w-RnzENIeY"
      },
      "id": "U0w-RnzENIeY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gte-large-en-v1.5"
      ],
      "metadata": {
        "id": "ACEJyxCeDWIf"
      },
      "id": "ACEJyxCeDWIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skllm.config import SKLLMConfig\n",
        "\n",
        "SKLLMConfig.set_gpt_url(\"http://127.0.0.1:11434/\")"
      ],
      "metadata": {
        "id": "t17NthF7H0qE"
      },
      "id": "t17NthF7H0qE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# Convert each text to lowercase and remove specific characters\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove new line and tab characters\n",
        "    text = text.replace('\\n', '').replace('\\t', '')\n",
        "    # Remove punctuation\n",
        "    for p in string.punctuation:\n",
        "        text = text.replace(p, '')\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to each text\n",
        "texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# Display the processed texts\n",
        "print(texts)"
      ],
      "metadata": {
        "id": "8TORYoEzzQya"
      },
      "id": "8TORYoEzzQya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skollama.models.ollama.vectorization import OllamaVectorizer\n",
        "\n",
        "vectorizer = OllamaVectorizer(batch_size=2) # batch_size is number of parallel tasks\n",
        "X_vec = vectorizer.fit_transform(texts)\n",
        "print(X_vec.shape)"
      ],
      "metadata": {
        "id": "zX6slqRUtwzX"
      },
      "id": "zX6slqRUtwzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#sc = StandardScaler()\n",
        "#X_vec = sc.fit_transform(X_vec)\n",
        "model = PCA(10)\n",
        "X_new = model.fit_transform(X_vec)\n",
        "X_new.shape"
      ],
      "metadata": {
        "id": "6SueSVRMvLzW"
      },
      "id": "6SueSVRMvLzW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "exp_var_pca = model.explained_variance_ratio_\n",
        "#\n",
        "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
        "# for visualizing the variance explained by each principal component.\n",
        "#\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "#\n",
        "# Create the visualization plot\n",
        "#\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_r7WemzbxHw8"
      },
      "id": "_r7WemzbxHw8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['x0'] = X_new[:,0]\n",
        "df['x1'] = X_new[:,1]\n",
        "df.plot(kind='scatter', x='x0', y='x1',\n",
        "    color=df.index.astype(int), hover_data=['president'],\n",
        "    backend='plotly')"
      ],
      "metadata": {
        "id": "QzmPwwT7vRmO"
      },
      "id": "QzmPwwT7vRmO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skllm.datasets import get_classification_dataset\n",
        "from skollama.models.ollama.classification.zero_shot import ZeroShotOllamaClassifier\n",
        "\n",
        "# Load a demo dataset\n",
        "X, y = get_classification_dataset()\n",
        "\n",
        "# Initialize the model and make the predictions\n",
        "clf = ZeroShotOllamaClassifier(model=\"llama3\")\n",
        "clf.fit(None, [\"positive\", \"negative\", \"neutral\"])\n",
        "#clf.fit(X, y)\n",
        "labels = clf.predict(X)"
      ],
      "metadata": {
        "id": "HSq5_uKyGhdN"
      },
      "id": "HSq5_uKyGhdN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Text\": X,\n",
        "    \"Predicted Label\": labels,\n",
        "    \"True Label\": y\n",
        "})\n",
        "df"
      ],
      "metadata": {
        "id": "DRbpcu9iLGxZ"
      },
      "id": "DRbpcu9iLGxZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Checkout [here](https://ollama.com/library) for more models. Also [here](https://andreaskarasenko.github.io/skollama-docs/) for more information about `scikit-ollama`."
      ],
      "metadata": {
        "id": "FBacVGYoEWSt"
      },
      "id": "FBacVGYoEWSt"
    },
    {
      "cell_type": "markdown",
      "id": "e7c279d9",
      "metadata": {
        "id": "e7c279d9"
      },
      "source": [
        "### ðŸ“š Further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a96b8ef",
      "metadata": {
        "id": "0a96b8ef"
      },
      "source": [
        "- [_Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}